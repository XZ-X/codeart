import random
import torch


class DataCollatorForMFC(object):

    def __init__(self, tokenizer, label2id, max_functions):
        self.tokenizer = tokenizer
        self.label2id = label2id
        self.max_functions = max_functions

    def __call__(
        self,
        examples
    ):
        
        batch = {
            'input_ids': [],
            'token_type_ids': [],
            'attention_mask': [],
            'labels': [],
            'sequence_mask': [],
            'all_labels': []
        }

        for example in examples:
            num_functions = 0
            sequence_mask = []
            for function in eval(example['functions'])[:self.max_functions]:
                funcstr = function['jtrans_function_string']
                encoded = self.tokenizer(funcstr, add_special_tokens=True,max_length=512,padding='max_length',truncation=True,return_tensors='pt')
                batch['input_ids'].append(encoded['input_ids'].squeeze(0))
                batch['token_type_ids'].append(encoded['token_type_ids'].squeeze(0))
                batch['attention_mask'].append(encoded['attention_mask'].squeeze(0))
                num_functions += 1
                sequence_mask.append(1)
            for _ in range(num_functions, self.max_functions):
                funcstr = function['jtrans_function_string']
                encoded = self.tokenizer(funcstr, add_special_tokens=True,max_length=512,padding='max_length',truncation=True,return_tensors='pt')
                batch['input_ids'].append(encoded['input_ids'].squeeze(0))
                batch['token_type_ids'].append(encoded['token_type_ids'].squeeze(0))
                batch['attention_mask'].append(encoded['attention_mask'].squeeze(0))
                sequence_mask.append(0)
            batch['labels'].append(self.label2id[random.sample(example['labels'], 1)[0]])
            batch["sequence_mask"].append(sequence_mask)
            batch["all_labels"].append(example['labels'])

        return {
            'input_ids': torch.stack(batch['input_ids']),
            'token_type_ids': torch.stack(batch['token_type_ids']),
            'attention_mask': torch.stack(batch['attention_mask']),
            'labels': torch.tensor(batch['labels'], dtype=torch.long).unsqueeze(1),
            'sequence_mask': torch.tensor(batch['sequence_mask'], dtype=torch.long),
            'all_labels': batch['all_labels']
        }


class DataCollatorForMFCMultilabel(object):

    def __init__(self, tokenizer, label2id, max_functions):
        self.tokenizer = tokenizer
        self.label2id = label2id
        self.max_functions = max_functions

    def __call__(
        self,
        examples
    ):
        
        batch = {
            'input_ids': [],
            'token_type_ids': [],
            'attention_mask': [],
            'labels': [],
            'sequence_mask': [],
        }

        for example in examples:
            num_functions = 0
            sequence_mask = []
            for function in eval(example['functions'])[:self.max_functions]:
                funcstr = function['jtrans_function_string']
                encoded = self.tokenizer(funcstr, add_special_tokens=True,max_length=512,padding='max_length',truncation=True,return_tensors='pt')
                batch['input_ids'].append(encoded['input_ids'].squeeze(0))
                batch['token_type_ids'].append(encoded['token_type_ids'].squeeze(0))
                batch['attention_mask'].append(encoded['attention_mask'].squeeze(0))
                num_functions += 1
                sequence_mask.append(1)
            for _ in range(num_functions, self.max_functions):
                funcstr = function['jtrans_function_string']
                encoded = self.tokenizer(funcstr, add_special_tokens=True,max_length=512,padding='max_length',truncation=True,return_tensors='pt')
                batch['input_ids'].append(encoded['input_ids'].squeeze(0))
                batch['token_type_ids'].append(encoded['token_type_ids'].squeeze(0))
                batch['attention_mask'].append(encoded['attention_mask'].squeeze(0))
                sequence_mask.append(0)
            labels = torch.zeros(len(self.label2id))
            for l in example['labels']:
                labels[self.label2id[l]] = 1
            batch['labels'].append(labels)
            batch["sequence_mask"].append(sequence_mask)

        return {
            'input_ids': torch.stack(batch['input_ids']),
            'token_type_ids': torch.stack(batch['token_type_ids']),
            'attention_mask': torch.stack(batch['attention_mask']),
            'labels': torch.stack(batch['labels']),
            'sequence_mask': torch.tensor(batch['sequence_mask'], dtype=torch.long),
        }